{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Week_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDUW3f4ws8GN",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMdKEr5Ds8GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKydiwpPs8Gd",
        "colab_type": "text"
      },
      "source": [
        "## CNN for MNIST\n",
        "\n",
        "In this section we will build a CNN with TensorFlow, and we will implement Minibatch Gradient Descent to train it on the MNIST dataset. The first step is the construction phase, building the TensorFlow graph. The second step is the execution phase, where you actually run the graph to train the model.\n",
        "\n",
        "First we need to import the tensorflow library. Then we must specify the number of inputs and outputs, and set the number of hidden neurons in each layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN-2ZX2HtSoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovlgYqDjs8Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOPK5TsCs8Gi",
        "colab_type": "code",
        "outputId": "82e8f23a-6017-46c0-c0a3-a8c08f984527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GEQwPa-s8Gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_conv1 = 64\n",
        "n_hidden1 = 512\n",
        "n_outputs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpEItDvs8Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = np.expand_dims(X_train,-1).astype(np.float32) / 255.0\n",
        "X_test = np.expand_dims(X_test,-1).astype(np.float32) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z6vZ02ys8Gr",
        "colab_type": "text"
      },
      "source": [
        "We use placeholder nodes to represent the training data and targets. The shape of X is only partially defined. We know that it will be a 4D tensor (number_instance, height, width, num_colour_channel), but we don’t know yet how many instances each training batch will contain. So the shape of X is (None, 28, 28, 1). Similarly, we know that y will be a 1D tensor with one entry per instance, but again we don’t know the size of the training batch at this point, so the shape is (None)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmJu_sPxs8Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CabJmRCas8Gw",
        "colab_type": "text"
      },
      "source": [
        "Now you need to create the one convolutional layer, one pooling layer, one flatten layer, one fully-connected layer, and finally one output layer (which is also fully connected)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbgT5FLBs8Gx",
        "colab_type": "code",
        "outputId": "386d98b9-8886-487e-8abe-4308ce2270e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    conv1 = tf.layers.conv2d(X, \n",
        "                             n_conv1, \n",
        "                             kernel_size = (7,7), \n",
        "                             strides=(3,3), \n",
        "                             name=\"conv1\", \n",
        "                             activation=tf.nn.relu)\n",
        "    \n",
        "    pool1 = tf.layers.max_pooling2d(conv1, \n",
        "                                    pool_size=(4,4), \n",
        "                                    strides=(4,4))\n",
        "    \n",
        "    flatten = tf.layers.flatten(pool1)\n",
        "\n",
        "    fc1 = tf.layers.dense(flatten, \n",
        "                          n_hidden1, \n",
        "                          name=\"fc1\",\n",
        "                          activation=tf.nn.relu)\n",
        "    \n",
        "    logits = tf.layers.dense(fc1, \n",
        "                             n_outputs, \n",
        "                             name=\"outputs\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-71a66e25a7e8>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-71a66e25a7e8>:11: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-71a66e25a7e8>:13: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-71a66e25a7e8>:18: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KNFY-1Ts8G0",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the neural network model ready to go, we need to define the cost function that we will use to train it. We will use cross entropy, cross entropy will penalize models that estimate a low probability for the target class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWUFE-qDs8G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2DJvngWs8G3",
        "colab_type": "text"
      },
      "source": [
        "We have the neural network model, we have the cost function, and now we need to define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWcLrqfDs8G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3h6DLGs8G6",
        "colab_type": "text"
      },
      "source": [
        "The last important step in the construction phase is to specify how to evaluate the model. We will simply use accuracy as our performance measure. First, for each instance, determine if the neural network’s prediction is correct by checking whether or not the highest logit corresponds to the target class. For this you can use the in_top_k() function. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network’s overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Lbvfsps8G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0rHRBxWs8G-",
        "colab_type": "text"
      },
      "source": [
        "And, as usual, we need to create a node to initialize all variables, and we will also create a Saver to save our trained model parameters to disk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpDxqibhs8HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08h60ok8s8HC",
        "colab_type": "text"
      },
      "source": [
        "Now we define the number of epochs that we want to run, as well as the size of the mini-batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LARbKcDXs8HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gfAyBOs8HG",
        "colab_type": "text"
      },
      "source": [
        "And now we can train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoIV3C1Os8HG",
        "colab_type": "code",
        "outputId": "01d73058-060d-4a95-c5da-b929a45f29ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "        \n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 0.92 Validation accuracy: 0.8896\n",
            "1 Batch accuracy: 0.94 Validation accuracy: 0.9272\n",
            "2 Batch accuracy: 0.96 Validation accuracy: 0.9412\n",
            "3 Batch accuracy: 0.92 Validation accuracy: 0.9486\n",
            "4 Batch accuracy: 0.96 Validation accuracy: 0.9562\n",
            "5 Batch accuracy: 0.98 Validation accuracy: 0.9588\n",
            "6 Batch accuracy: 1.0 Validation accuracy: 0.9646\n",
            "7 Batch accuracy: 1.0 Validation accuracy: 0.967\n",
            "8 Batch accuracy: 0.98 Validation accuracy: 0.9684\n",
            "9 Batch accuracy: 0.94 Validation accuracy: 0.9678\n",
            "10 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
            "11 Batch accuracy: 0.98 Validation accuracy: 0.9734\n",
            "12 Batch accuracy: 0.96 Validation accuracy: 0.973\n",
            "13 Batch accuracy: 0.96 Validation accuracy: 0.971\n",
            "14 Batch accuracy: 0.98 Validation accuracy: 0.9732\n",
            "15 Batch accuracy: 0.94 Validation accuracy: 0.9744\n",
            "16 Batch accuracy: 1.0 Validation accuracy: 0.9766\n",
            "17 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
            "18 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
            "19 Batch accuracy: 0.96 Validation accuracy: 0.9782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouR3znp7s8HJ",
        "colab_type": "text"
      },
      "source": [
        "This code opens a TensorFlow session, and it runs the init node that initializes all the variables. Then it runs the main training loop: at each epoch, the code iterates through a number of mini-batches that corresponds to the training set size. Each mini-batch is fetched, and then the code simply runs the training operation, feeding it the current mini-batch input data and targets. Next, at the end of each epoch, the code evaluates the model on the last mini-batch and on the full training set, and it prints out the result. Finally, the model parameters are saved to disk.\n",
        "\n",
        "Now that the neural network is trained, you can use it to make predictions. To do that, you can reuse the same construction phase, but change the execution phase like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyaQaQLps8HJ",
        "colab_type": "code",
        "outputId": "9ec4cf4e-b648-4aa0-9f28-d16a922c081b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
        "    X_new_scaled = X_test[:20]\n",
        "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
        "    y_pred = np.argmax(Z, axis=1)\n",
        "    \n",
        "print(\"Predicted classes:\", y_pred)\n",
        "print(\"Actual classes:   \", y_test[:20])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "Predicted classes: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
            "Actual classes:    [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMfkYh7Qs8HO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpeZIHzKs8HS",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "**Task 1:** Train your own CNN on the MNIST dataset and see if you can get over 98% precision.\n",
        "\n",
        "Hints: Use deeper CNNs; Use more advanced optimiser rather than SGD (e.g., Adam); Use dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSLejjls8HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}